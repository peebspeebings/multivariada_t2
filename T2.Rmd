---
title: "Análise Multivariada - Trabalho 2"
author: "Pedro de Oliveira Macedo"
date: '2023-09-10'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Carregar dados

```{r}
dados_originais <- read.csv("spotify_data.csv")
aux <- sample(1:nrow(dados_originais), size=nrow(dados_originais)*0.005)
dados <- dados_originais[aux,-c(1:4)]
#variaveis categoricas: c(2,3,6,8,16)
```

## Carregar pacotes

```{r, message=F, warning=F}
library(dplyr)
library(ggplot2)
library(GGally)
library(corrplot)
library(factoextra)
library(gridExtra)
```


## Descritiva

```{r}
corrplot(cor(dados[,-c(2,3,6,8,16)]))
```
Espera-se que as variáveis acousticness, energy, loudness e valence sejam cruciais no agrupamento de observações.


## Distâncias

### Distancia pearson

```{r}
res.dist <- get_dist(dados[,-c(2,3,6,8,16)], method = "pearson") # Correlation-based distance method
fviz_dist(res.dist, lab_size = 8) # Visualize the dissimilarity matrix
```

### Distancia euclidiana

```{r}

res.dist2 <- get_dist(wines, method = "euclidian") # Correlation-based distance method
fviz_dist(res.dist2, lab_size = 8) # Visualize the dissimilarity matrix
```

### Distancia Manhattan

```{r}

res.dist3 <- get_dist(wines, method = "manhattan") # Correlation-based distance method
fviz_dist(res.dist3, lab_size = 8) # Visualize the dissimilarity matrix
```

### Distancia Minkowski

```{r}

res.dist4 <- get_dist(wines, method = "minkowski") # Correlation-based distance method
fviz_dist(res.dist4, lab_size = 8) # Visualize the dissimilarity matrix

```

### Grafico interativo

```{r}

#install.packages("d3heatmap")
library(d3heatmap)
d3heatmap(scale(wines), colors = "RdYlBu")

```

## K-Means Clustering

```{r}
# normalização das variáveis 
dadosNorm <- as.data.frame(scale(dados[,-c(2,3,6,8,16)]))

set.seed(1234)
dados_k2 <- kmeans(dadosNorm, centers = 2)
```

```{r}
dados_k2$size
dados_k2$size/nrow(dados)
```
O primeiro cluster tem 4477 (0.77%) observações, enquanto que o segundo tem 1321 (0.23%).


```{r}
aggregate(dados[,-c(2,3,6,8,16)], by=list(dados_k2$cluster), mean)
```


### Otimizando k

```{r, warning=F}
# quantos clusters? 

bss <- numeric()
wss <- numeric()


for(i in 1:10){
  
  # For each k, calculate betweenss and tot.withinss
  bss[i] <- kmeans(dadosNorm, centers=i)$betweenss
  wss[i] <- kmeans(dadosNorm, centers=i)$tot.withinss
  
}

# Between-cluster sum of squares vs Choice of k
p3 <- qplot(1:10, bss, geom=c("point", "line"), 
            xlab="Number of clusters", ylab="Between-cluster sum of squares") +
  scale_x_continuous(breaks=seq(0, 10, 1)) +
  theme_bw()

# Total within-cluster sum of squares vs Choice of k
p4 <- qplot(1:10, wss, geom=c("point", "line"),
            xlab="Number of clusters", ylab="Total within-cluster sum of squares") +
  scale_x_continuous(breaks=seq(0, 10, 1)) +
  theme_bw()

# Subplot
grid.arrange(p3, p4, ncol=2)
```
A partir das somas de quadrados, 2 ou 3 clusters seriam ideais. O ganho da inclusão de mais clusters é em geral similar, com exceção à diferença entre 6 e 7 clusters.

### Plots

```{r}

```

